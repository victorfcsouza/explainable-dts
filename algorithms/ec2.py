"""
    Implementation of the CART algorithm to train decision tree classifiers.
"""

import numpy as np

from algorithms.default_algorithm import DefaultClassifier
from tree import tree


class EC2(DefaultClassifier):
    def __init__(self, max_depth=None, min_samples_stop=0):
        super().__init__(max_depth, min_samples_stop)

    @staticmethod
    def _number_pairs(y):
        """
        Returns the number of pairs that have different classes
        """
        count = 0
        y_sorted = sorted(y)

        i = 1
        j = 0
        n = len(y)
        while i < n:
            while i < n and y_sorted[i] == y_sorted[i - 1]:
                i += 1
            count += (i - j) * (n - i)
            j = i
            i += 1
        return count

    def _num_pairs_left_right(self, values, threshold, classes):
        """
        
        @param values: values of a attribute t
        @param threshold:  threshold to set
        @param classes: classes in the same order as values
        @return:  pairs left, pairs_left_right, pairs_right
        """

        all_pairs = self._number_pairs(classes)
        n = len(values)
        index_left = []
        index_right = []
        for idx in range(n):
            if values[idx] <= threshold:
                index_left.append(idx)
            else:
                index_right.append(idx)
        classes_left = [classes[idx] for idx in index_left]
        classes_right = [classes[idx] for idx in index_right]

        pairs_left = self._number_pairs(classes_left)
        pairs_right = self._number_pairs(classes_right)
        pairs_left_right = all_pairs - pairs_left - pairs_right

        return len(classes_left), pairs_left, pairs_left_right, pairs_right, len(classes_right)

    def _best_split(self, X, y, feature_index_occurrences=None, modified_factor=1, father_feature=None):
        """Find the best split for a node.

        "Best" means that score is the largest.

        To find the best split, we loop through all the features, and consider all the
        midpoints between adjacent training samples as possible thresholds. We compute
        the score generated by that particular feature/threshold
        pair, and return the pair with greatest score

        Returns:
            best_idx: Index of the feature for best split, or None if no split is found.
            best_thr: Threshold to use for the split, or None if no split is found.
        """
        # Need at least two elements to split a node.
        m = y.size
        if m <= 1:
            return None, None

        # Count of each class in the current node.
        num_parent = [np.sum(y == c) for c in range(self.n_classes_)]

        # Gini of current node.
        best_score = 0
        best_idx, best_thr = None, None
        all_pairs = self._number_pairs(y)

        # Loop through all features.
        for idx in range(self.n_features_):
            # Sort data along selected feature.
            thresholds, classes = zip(*sorted(zip(X[:, idx], y)))

            num_left = [0] * self.n_classes_
            num_right = num_parent.copy()
            pairs_left = 0
            pairs_right = all_pairs
            for i in range(1, m):  # possible split positions
                c = classes[i - 1]
                num_left[c] += 1
                num_right[c] -= 1
                pairs_left += sum(num_left) - num_left[c]
                pairs_right -= sum(num_right) - num_right[c]
                pairs_left_right = all_pairs - pairs_left - pairs_right

                # The following condition is to make sure we don't try to split two
                # points with identical values for that feature, as it is impossible
                # (both have to end up on the same side of a split).
                if thresholds[i] == thresholds[i - 1]:
                    continue
                thr = (thresholds[i] + thresholds[i - 1]) / 2
                score = len(num_left) * (pairs_left_right + pairs_right) + \
                        len(num_right) * (pairs_left_right * pairs_left)

                if score > best_score:
                    best_score = score
                    best_idx = idx
                    best_thr = thr

        return best_idx, best_thr

    def _grow_tree(self, X, y, depth=0, feature_index_occurrences=None, modified_factor=1, calculate_gini=False,
                   father_feature=None, gamma_factor=None):
        """Build a decision tree by recursively finding the best split."""
        # Population for each class in current node. The predicted class is the one with
        # largest population.
        num_samples_per_class = [np.sum(y == i) for i in range(self.n_classes_)]
        predicted_class = np.argmax(num_samples_per_class)
        node = tree.Node(
            num_samples=y.size,
            num_samples_per_class=num_samples_per_class,
            predicted_class=predicted_class,
            feature_index_occurrences=feature_index_occurrences.copy()
        )
        if calculate_gini:
            node.gini = self._gini(y)

        # Split recursively until maximum depth is reached.
        if depth < self.max_depth and node.num_samples >= self.min_samples_stop:
            idx, thr = self._best_split(X, y, feature_index_occurrences=feature_index_occurrences,
                                        modified_factor=modified_factor, father_feature=father_feature)
            if idx is not None:
                indices_left = X[:, idx] <= thr
                X_left, y_left = X[indices_left], y[indices_left]
                X_right, y_right = X[~indices_left], y[~indices_left]
                node.feature_index = idx
                node.threshold = thr
                node.feature_index_occurrences[idx] += 1
                node.left = self._grow_tree(X_left, y_left, depth + 1,
                                            feature_index_occurrences=node.feature_index_occurrences.copy(),
                                            modified_factor=modified_factor, calculate_gini=calculate_gini,
                                            father_feature=node.feature_index)
                node.right = self._grow_tree(X_right, y_right, depth + 1,
                                             feature_index_occurrences=node.feature_index_occurrences.copy(),
                                             modified_factor=modified_factor, calculate_gini=calculate_gini,
                                             father_feature=node.feature_index)
        return node
