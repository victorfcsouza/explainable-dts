"""
    Implementation of CART algorithm that optimizes the number of pairs
"""
import numpy as np
import math

from algorithms.cart import CART
from algorithms.serdt_base import SERDTBase
from tree import tree


class CARTPairs(CART, SERDTBase):
    def __init__(self, max_depth=None, min_samples_stop=0):
        super().__init__(max_depth, min_samples_stop)

    def _get_best_threshold(self, X, y, a, classes_parent, node_pairs, node_product):
        """
            Get the best threshold given an attribute.
        """
        m = y.size  # tirar
        thresholds, classes_thr = zip(*sorted(zip(X[:, a], y)))
        classes_left = [0] * self.n_classes_
        classes_right = classes_parent.copy()
        pairs_left = 0
        pairs_right = node_pairs
        t_best = None
        cost_best = math.inf

        for i in range(1, m):
            node_class = classes_thr[i - 1]
            classes_left[node_class] += 1
            classes_right[node_class] -= 1
            pairs_left += sum([classes_left[j] for j in range(self.n_classes_) if j != node_class])
            pairs_right -= sum([classes_right[j] for j in range(self.n_classes_) if j != node_class])
            if thresholds[i] == thresholds[i - 1]:
                continue
            threshold = (thresholds[i] + thresholds[i - 1]) / 2
            cost = pairs_left / i + pairs_right / (m - i)  # Gini in terms of pairs
            if cost < cost_best:
                cost_best = cost
                t_best = threshold
        return t_best, cost_best

    def _best_split(self, X, y, feature_index_occurrences=None, modified_factor=1):
        """Find the best split for a node.

        "Best" means that the average impurity of the two children, weighted by their
        population, is the smallest possible. Additionally, it must be less than the
        impurity of the current node.

        To find the best split, we loop through all the features, and consider all the
        midpoints between adjacent training samples as possible thresholds. We compute
        the Gini impurity of the split generated by that particular feature/threshold
        pair, and return the pair with smallest impurity.

        Returns:
            best_idx: Index of the feature for best split, or None if no split is found.
            best_thr: Threshold to use for the split, or None if no split is found.
        """
        # Need at least two elements to split a node.
        m = y.size
        if m <= 1:
            return None, None

        # Best cost (Gini in terms of pairs)
        best_cost = math.inf

        node_pairs = self._number_pairs(y)
        node_product = node_pairs * y.size
        classes_parent = [np.sum(y == c) for c in range(self.n_classes_)]
        best_idx, best_thr = None, None

        # Loop through all features.
        for idx in range(self.n_features_):
            t_best_a, cost_best_a = self._get_best_threshold(X, y, idx, classes_parent, node_pairs, node_product)
            if cost_best_a < best_cost:
                best_cost = cost_best_a
                best_idx = idx
                best_thr = t_best_a

        return best_idx, best_thr

    def _grow_tree(self, X, y, depth=0, feature_index_occurrences=None, modified_factor=1, calculate_gini=True):
        """Build a decision tree by recursively finding the best split."""
        # Population for each class in current node. The predicted class is the one with
        # largest population.
        num_samples_per_class = [np.sum(y == i) for i in range(self.n_classes_)]
        predicted_class = np.argmax(num_samples_per_class)
        node = tree.Node(
            num_samples=y.size,
            num_samples_per_class=num_samples_per_class,
            predicted_class=predicted_class,
            feature_index_occurrences=feature_index_occurrences.copy()
        )
        if calculate_gini:
            node.gini = self._gini(y)

        # Split recursively until maximum depth is reached.
        if depth < self.max_depth and node.num_samples >= self.min_samples_stop:
            idx, thr = self._best_split(X, y, feature_index_occurrences=feature_index_occurrences,
                                        modified_factor=modified_factor)
            if idx is not None:
                indices_left = X[:, idx] < thr
                X_left, y_left = X[indices_left], y[indices_left]
                X_right, y_right = X[~indices_left], y[~indices_left]
                node.feature_index = idx
                node.threshold = thr
                node.feature_index_occurrences[idx] += 1
                node.left = self._grow_tree(X_left, y_left, depth + 1,
                                            feature_index_occurrences=node.feature_index_occurrences.copy(),
                                            modified_factor=modified_factor, calculate_gini=calculate_gini)
                node.right = self._grow_tree(X_right, y_right, depth + 1,
                                             feature_index_occurrences=node.feature_index_occurrences.copy(),
                                             modified_factor=modified_factor, calculate_gini=calculate_gini)
        return node

    def fit(self, X, y, modified_factor=1):
        """Build decision tree classifier."""
        self.n_classes_ = len(set(y))  # classes are assumed to go from 0 to n-1
        self.n_samples = len(y)
        self.n_features_ = X.shape[1]
        feature_index_occurrences = [0] * self.n_features_
        self.tree_ = self._grow_tree(X, y, feature_index_occurrences=feature_index_occurrences,
                                     modified_factor=modified_factor, calculate_gini=False)
